#!/bin/bash 

#read static variables
inst_root=$(dirname $(readlink -f $0))
source $inst_root/a_env_conf

if [[ $# -lt 1 ]]
then
  echo "Please give the name of a file to be uploaded to allas as an argument of this command."
  echo ""
  exit 1
fi

# local variables
user=($USER)
tmp_file=("not_defined")
bucket_name=("not_defined")
print_help=(0)
os_project_name=("none")
input_def=("")
mode=("swift")
tmp_dir=("${tmp_root}/a_put_$$_tmp")
abspath() { old=`pwd`;new=$(dirname "$1");if [ "$new" != "." ]; then cd $new; fi;file=`pwd`/$(basename "$1");cd $old;echo $file; }

#Process command line
while [[ $# -ge 1 ]]
do
  case "$1" in
             '--bucket' | '-b' )
             # query file
                  bucket_name=($2)
                  shift
                  shift
                ;;
             '--project' | '-p' )
                  os_project_name=($2)
                  shift
                  shift
                ;;
             '--os_file' | '-o' )
                  partial_path=$(dirname $2)
                  tmp_file=$(basename $2)
                  shift
                  shift
                ;;
            '--s3cmd' )
                  mode=("s3cmd")
                  shift
                ;;
             '-h' | '--help' )
                  print_help=(1)
                  shift
                ;;
             *)
                   input_def=("$input_def $1")
                   shift                       # No more switches
                ;;
    esac
done

if [ $print_help -eq 1 ]; then
cat <<EOF
This tools is used to upload data from the disk environment of Taito and Puhti to 
allas storage environmnet. The basic syntax of the command is:

   a_put directory_or_file

By defualt this tool performs followiong operations:

1. Ensures that you have working connection to Allas storage service and 
defines the project that will be used to store the data.

2. In case of directory, the content of the directory is collected into single file
(using tar command).

3. Data is compressed using zstdmt command unles you diable compression  with option: --nc

4. The compressed data is uploaded to Allas using rclone command and swift protocol.

The location were data is stored in allas can be defined with options
--bucket (-b) and --os_file (-b).

 The default option is that data that locates in 
  a) $WRKDIR is uploaded to bucket:  username-poject_number-SCRATCH
and the data that locates in
  b) $HOME is uploaded to: username-poject_number-HOME

  c) in other cases the data uploaded to: username-poject_number-MISC

For example for user kkaytaj belonging in project_12345,data locatioing in home directory
will be uploaded to bucket:  kkayttaj-12345-HOME.

THe compressed dataset will be stored as one object. The object name depends on the
file name and location.  The logic used is that the possible subdirectory path in Taito is included 
in the object name. E.g. a file called test_1.txt in $WRKDIR can be stored with commands:

   cd $WRKDIR
   a_put test_1.txt

In this case the file is stored to bucket: kkayttaj-12345-SCRATCH 
as object: test_1.txt.zst

If you have another file called test_1.txt that locates in location $WRKDIR/project2/sample3
you can store it with commands:
   
  cd $WRKDIR/project2/sample3
  a_put test_1.txt
  
Or commmands
  cd $WRKDIR
  a_put project2/sample3/test_1.txt

In this case the file is stored to bucket: kkayttaj-12345-SCRATCH 
as object:  project2/sample3/test_1.txt.zst

Options

-b, --bucket <bucket_name>  Define a name of the bucket into which the data is uploaded.

-p, --project <project_ID>  Upload data into buckets of the defined project in stead of the currently configured project.

-o, --os_file <object_name> Define a name for the new object top be created.

--s3cmd                     Use s3cmd protocol for upoload in stead of swift protocol.

-n, --nc                    Do not compress the data that will be uploaded.

-h, --help                  Print this help.


Related commands: a_find, a_get, a_delete, a_info
EOF

exit 

fi 

#Configure rclone
if [ ! -e $HOME/.config/rclone ]
then 
  mkdir -p  $HOME/.config/rclone
fi
if [ -e $HOME/.config/rclone/rclone.conf ]
then 
   rc_check=$(grep -c $storage_server $HOME/.config/rclone/rclone.conf)
   if [ $rc_check -lt 1 ]
   then
      echo '[pouta]' >>  $HOME/.config/rclone/rclone.conf
      echo 'type = swift'  >>  $HOME/.config/rclone/rclone.conf
      echo 'env_auth = true'   >>  $HOME/.config/rclone/rclone.conf
   fi
else
   echo '[pouta]' >>  $HOME/.config/rclone/rclone.conf
   echo 'type = swift'  >>  $HOME/.config/rclone/rclone.conf
   echo 'env_auth = true'   >>  $HOME/.config/rclone/rclone.conf 
fi

#Assign project to be used if not defined 
if [ $os_project_name == "none" ]
then
  if [ -e $HOME/.allas_default ]
  then
     source $HOME/.allas_default
     if [[ $os_project_name != $OS_PROJECT_NAME ]]
     then 
        echo "Switching allas configuration to use project $os_project_name"
        source $allas_conf_path -user $user $os_project_name
     fi 
  else
     echo "Default project is not defined"
     source $allas_conf_path -user $user
     echo "os_project_name=$OS_PROJECT_NAME" > $HOME/.allas_default
     echo "Default allas project is stored to  \$HOME/.allas_default"
     echo ""
  fi
fi
source $HOME/.allas_default

#Check if connection works
if [[ $mode == "swift" ]]
then
  test=$(swift stat 2> /dev/null | grep -c "Account:")
  if [[ $test -lt 1 ]]
  then 
    echo "No connection to Allas!"
    echo "Please try setting the the connection again."
    exit 1
  fi 
fi

#source /appl/opt/allas_conf
#input=("$1")

#check free space in $WRKDIR
#quota_s=($(lfs quota -q -u $USER $WRKDIR))
#free_space=$(expr ${quota_s[2]} - ${quota_s[1]})

echo "Files to be uploaded: $input_def"

#set default bucket
project_label=$(echo ${os_project_name} |  sed -e s/"project_"/""/g)
if [[ "$bucket_name" == "not_defined" ]]
then
   bucket_name=("${user}-${project_label}-pub")
fi 
echo "Bucket: $bucket_name"





mkdir $tmp_dir

for input in $input_def
do 
  echo "Processing: $input"
  if [ ! -e $input ] ; then
    echo "File: $input does not exist!"
    exit 1
  fi
  
  if [[ $(file -b $input ) == "directory" ]]
  then
    echo "This command can only be used to publish files, not directories."
    exit 1
  fi
 
  #check that file name does not end with _meta
  if [[ ${input:(-5):5} == "_meta" ]]; then
    echo "Found a file/directoryname which ends with _meta"
    echo "  $input"
    echo ""
    echo "Please rename this file as it will mix up a the metadata management of a_put"
    exit 1
  fi

  file_path=$(abspath $input)

  echo "Checking total size of $input. Please wait."
  tot_size=$(du -s $input | cut -f1)
  #echo $tot_size

  #tmp file name. Depens on compression and is file is a directory
  if [  $tmp_file == "not_defined" ]
  then
     tmp_file=($(basename $input | tr " " "_" ))
  fi


  #Check if stored file already exitst 
  if [[ $(rclone ls ${storage_server}:${bucket_name}/${partial_path}/$tmp_file 2> /dev/null | wc -c) > 2 ]]
  then
    echo ""
    echo "A file/directory with the same name has already been uploaded into"
    echo "bucket $bucket_name in $storage_server"
    echo ""
    echo "rclone ls ${storage_server}:${bucket_name}${partial_path}/$tmp_file  "
    rclone ls ${storage_server}:${bucket_name}${partial_path}/$tmp_file  
    echo ""
    echo "Remove the old file if you wish to upload a new version of $input to $storage_server."
    echo "Optionally you can define different bucket name using option -bucket ."
    exit 1
  fi

  #collect and count metadata
  echo "user: $user" >> ${tmp_dir}/${tmp_file}_meta
  echo "host: $(hostname)" >> ${tmp_dir}/${tmp_file}_meta
  echo "original_location: $file_path" >>  ${tmp_dir}/${tmp_file}_meta
  echo ""
  ls -l $input >>  ${tmp_dir}/${tmp_file}_meta
  
  if [[ $tot_size -gt $max_size ]]
  then 
    echo "This file or directory is too big for this tool"
    echo "Total size: ${tot_size}K"
    echo "Please use swift or rclone command to upload the data to allas"
    rm -f ${tmp_dir}/${tmp_file}_meta
    rmdir ${tmp_dir} 
    exit 1
  fi 

#  if [[ $tot_size -gt $free_space ]]
#  then 
#    echo "There is not enough space for the temporary files."
#    echo "$input contains $num_files files or directories that take ${tot_size}K of disk space"
#    echo "Available free space is ${free_space}K"
#    echo ""
#    rm -f ${tmp_dir}/${tmp_file}_meta
#    rmdir ${tmp_dir}
#    exit 1
#  fi

  #this is not a good approach.
  #test if a symbolic link would do
  cp $input ${tmp_dir}/$tmp_file  


  #upload

  if [ $mode == "swift" ]
  then
    # For less than 5GB files rclone is used for uploading
    
    echo "Uploading data to allas."
    # echo "rclone copy --progress ${tmp_dir}/$tmp_file ${storage_server}:${bucket_name}/${partial_path}"
    rclone copy --progress ${tmp_dir}/$tmp_file ${storage_server}:${bucket_name}/${partial_path}
    exitcode=$?
    if [ $exitcode -ne 0 ]; then
       echo ""
       echo "File upload for $infile failed"
       rclone deletefile ${storage_server}:${bucket_name}/${partial_path}/$tmp_file
       rm -f ${tmp_dir}/$tmp_file 
       rm -f ${tmp_dir}/${tmp_file}_meta
       rmdir ${tmp_dir}
       exit 1
    fi

    # rclone md5sums can be calculated only for files that are smaller than 5GB
    if [[ $tmp_size -lt 5000000 ]]
    then
       echo "Confirming upload..."
       #checksums for local and allas files 
       sum1=($(md5sum ${tmp_dir}/$tmp_file))
       sum2=($(rclone md5sum  ${storage_server}:${bucket_name}/${partial_path}/$tmp_file))
 
       #check is cheksums match 
       if [[ ${sum1[0]} !=  ${sum2[0]} ]]
       then 
         echo "Upload of $input was not successfull!"
         echo "Cleaning the failed upload"
         rclone deletefile ${storage_server}:${bucket_name}/${partial_path}/$tmp_file
         rm -f ${tmp_dir}/$tmp_file 
         rm -f ${tmp_dir}/${tmp_file}_meta
         rmdir ${tmp_dir}
         exit 1
       fi
       echo "$input OK"
    fi
  fi


  #echo "Remopving temporary file"
  rm -f  ${tmp_dir}/$tmp_file

  #update metadata
  echo ""
  echo "Adding metadata for uploaded $input"
  #echo "rclone copy ./${tmp_file}_meta ${storage_server}:${bucket_name}/${partial_path}" 
  rclone copy  ${tmp_dir}/${tmp_file}_meta ${storage_server}:${bucket_name}/${partial_path}
  
  rm -f ${tmp_dir}/${tmp_file}_meta

  echo "$input uploaded to ${bucket_name}"

  echo "Publick link: https://object.pouta.csc.fi/${bucket_name}/${partial_path}/$tmp_file"
  tmp_file=("not_defined")

done

#Publish and rebuild index file

echo '<html>' >  ${tmp_dir}/index.html
for i in $(swift list $bucket_name | grep -v '_meta$' )
do 
echo '<li><a href="'$i'">'$i'</a></li>' >> ${tmp_dir}/index.html
done
echo '</html>' >>  ${tmp_dir}/index.html
rclone copy  ${tmp_dir}/index.html ${storage_server}:${bucket_name}/

rm -f   ${tmp_dir}/index.html

#set access control
swift post ${bucket_name} --read-acl ".r:*,.rlistings"

rmdir  ${tmp_dir}
echo ""
echo "Upload ready"
exit 0
